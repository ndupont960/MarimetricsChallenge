{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0Wut2G5kz2Z"
      },
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# Segment Anything Model (SAM)\n",
        "\n",
        "---\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/segment-anything) [![arXiv](https://img.shields.io/badge/arXiv-2304.02643-b31b1b.svg)](https://arxiv.org/abs/2304.02643)\n",
        "\n",
        "Segment Anything Model (SAM): a new AI model from Meta AI that can \"cut out\" any object, in any image, with a single click. SAM is a promptable segmentation system with zero-shot generalization to unfamiliar objects and images, without the need for additional training. This notebook is an extension of the [official notebook](https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb) prepared by Meta AI.\n",
        "\n",
        "![segment anything model](https://media.roboflow.com/notebooks/examples/segment-anything-model-paper.png)\n",
        "\n",
        "## Complementary Materials\n",
        "\n",
        "---\n",
        "\n",
        "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/D-D6ZmadzPE) [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-use-segment-anything-model-sam)\n",
        "\n",
        "We recommend that you follow along in this notebook while reading the blog post on Segment Anything Model.\n",
        "\n",
        "![segment anything model blogpost](https://media.roboflow.com/notebooks/examples/segment-anything-model-blogpost.png)\n",
        "\n",
        "## Pro Tip: Use GPU Acceleration\n",
        "\n",
        "If you are running this notebook in Google Colab, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times.\n",
        "\n",
        "## Steps in this Tutorial\n",
        "\n",
        "In this tutorial, we are going to cover:\n",
        "\n",
        "- **Before you start** - Make sure you have access to the GPU\n",
        "- Install Segment Anything Model (SAM)\n",
        "- Download Example Data\n",
        "- Load Model\n",
        "- Automated Mask Generation\n",
        "- Generate Segmentation with Bounding Box\n",
        "- Segment Anything in Roboflow Universe Dataset\n",
        "\n",
        "## Let's begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2s3TnW4nhjC"
      },
      "source": [
        "## Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Frcrk09FhJeV",
        "outputId": "d51202e7-2821-4450-debe-19b42d8346f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan 29 11:48:03 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0              29W /  70W |   2679MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iov3yhoRnxG2"
      },
      "source": [
        "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgS8jFPMnj5h",
        "outputId": "c789b0e3-a3f0-45ea-c89c-dd90c0dc06b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOME: /content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN3DPGZSn57p"
      },
      "source": [
        "## Install Segment Anything Model (SAM) and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H9YruJen0Q8",
        "outputId": "2844db27-bbb0-4c2e-d9ec-3cedab8cf953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "G3CtzYroC2Lb"
      },
      "outputs": [],
      "source": [
        "!pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision==0.23.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VeYIWh1iDWW"
      },
      "source": [
        "### Download SAM weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Aszw1OxBwowI"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {HOME}/weights\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P {HOME}/weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxoFmhsHw_fG",
        "outputId": "ed2bfe1a-07c6-48e7-8c64-bdc17fa17255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/weights/sam_vit_h_4b8939.pth ; exist: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
        "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIlYzcqqpZdc"
      },
      "source": [
        "## Download Example Data\n",
        "\n",
        "**NONE:** Let's download few example images. Feel free to use your images or videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PKv4fEE1pdKE"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {HOME}/data\n",
        "\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg -P {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-2.jpeg -P {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-3.jpeg -P {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-4.jpeg -P {HOME}/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlhbd_f4xfiJ"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "t6_9PSZupghA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_TYPE = \"vit_h\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "n41g6y-Zx-9x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f6001b-bc2f-4ff9-8b94-7c0eb2a38e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
          ]
        }
      ],
      "source": [
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi3C4uDWo10h"
      },
      "source": [
        "## Automated Mask Generation\n",
        "\n",
        "To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class. Set the path below to the SAM checkpoint. Running on CUDA and with the default model is recommended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CtymFaiKyQ57"
      },
      "outputs": [],
      "source": [
        "mask_generator = SamAutomaticMaskGenerator(sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "I0Pm0RYArgm9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# IMAGE_NAME = \"dog.jpeg\"\n",
        "# IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)\n",
        "\n",
        "IMAGE_NAME = \"stolt-sea-farm-tocha.png\"\n",
        "IMAGE_PATH = os.path.join(\"C:/Users/ndupo/Documents/Code projects/Python/Marimetrics project/\", IMAGE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdgL88fUuelk"
      },
      "source": [
        "### Generate masks with SAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "u34UjLT8o7iC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "276feb74-7c05-4720-cc17-a9219a97b02d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-d07172cb4e45>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage_bgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimage_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_bgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msam_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_rgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import supervision as sv\n",
        "\n",
        "image_bgr = cv2.imread(IMAGE_PATH)\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "sam_result = mask_generator.generate(image_rgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUNhAvdPjZ-Y"
      },
      "source": [
        "### Output format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxO265XOymA2"
      },
      "source": [
        "`SamAutomaticMaskGenerator` returns a `list` of masks, where each mask is a `dict` containing various information about the mask:\n",
        "\n",
        "* `segmentation` - `[np.ndarray]` - the mask with `(W, H)` shape, and `bool` type\n",
        "* `area` - `[int]` - the area of the mask in pixels\n",
        "* `bbox` - `[List[int]]` - the boundary box of the mask in `xywh` format\n",
        "* `predicted_iou` - `[float]` - the model's own prediction for the quality of the mask\n",
        "* `point_coords` - `[List[List[float]]]` - the sampled input point that generated this mask\n",
        "* `stability_score` - `[float]` - an additional measure of mask quality\n",
        "* `crop_box` - `List[int]` - the crop of the image used to generate this mask in `xywh` format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRiwoYNEzbBN"
      },
      "outputs": [],
      "source": [
        "print(sam_result[0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkNDZqBEj5Cr"
      },
      "source": [
        "### Results visualisation with Supervision\n",
        "\n",
        "As of version `0.5.0` Supervision has native support for SAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdT3xT2AkS4g"
      },
      "outputs": [],
      "source": [
        "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "detections = sv.Detections.from_sam(sam_result=sam_result)\n",
        "\n",
        "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=[image_bgr, annotated_image],\n",
        "    grid_size=(1, 2),\n",
        "    titles=['source image', 'segmented image']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsdFDDQnjhkP"
      },
      "source": [
        "### Interaction with segmentation results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS_WhY60XMNL"
      },
      "outputs": [],
      "source": [
        "masks = [\n",
        "    mask['segmentation']\n",
        "    for mask\n",
        "    in sorted(sam_result, key=lambda x: x['area'], reverse=True)\n",
        "]\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=masks,\n",
        "    grid_size=(8, int(len(masks) / 8)),\n",
        "    size=(16, 16)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXKPiidy9nwH"
      },
      "source": [
        "## Generate Segmentation with Bounding Box\n",
        "\n",
        "The `SamPredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU1SN8_WCLny"
      },
      "outputs": [],
      "source": [
        "mask_predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcHT2Jr0u0_K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "IMAGE_NAME = \"dog.jpeg\"\n",
        "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS27Xlnb7MAj"
      },
      "source": [
        "### Draw Box\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSwoXkDi9uVD"
      },
      "outputs": [],
      "source": [
        "# helper function that loads an image before adding it to the widget\n",
        "\n",
        "import base64\n",
        "\n",
        "def encode_image(filepath):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        image_bytes = f.read()\n",
        "    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
        "    return \"data:image/jpg;base64,\"+encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFGBhRQNC0-H"
      },
      "source": [
        "**NOTE:** Execute cell below and use your mouse to draw bounding box on the image 👇"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zieb7wDZCoj2"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = True\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "from jupyter_bbox_widget import BBoxWidget\n",
        "\n",
        "widget = BBoxWidget()\n",
        "widget.image = encode_image(IMAGE_PATH)\n",
        "widget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSAhAXOULj0t"
      },
      "outputs": [],
      "source": [
        "widget.bboxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsy-GikiuX5l"
      },
      "source": [
        "### Generate masks with SAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqxt0CdkFUf8"
      },
      "source": [
        "**NOTE:** `SamPredictor.predict` method takes `np.ndarray` `box` argument in `[x_min, y_min, x_max, y_max]` format. Let's reorganise your data first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYyhnP4xFO5_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# default_box is going to be used if you will not draw any box on image above\n",
        "default_box = {'x': 68, 'y': 247, 'width': 555, 'height': 678, 'label': ''}\n",
        "\n",
        "box = widget.bboxes[0] if widget.bboxes else default_box\n",
        "box = np.array([\n",
        "    box['x'],\n",
        "    box['y'],\n",
        "    box['x'] + box['width'],\n",
        "    box['y'] + box['height']\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGxKHiK2uqtE"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "image_bgr = cv2.imread(IMAGE_PATH)\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "mask_predictor.set_image(image_rgb)\n",
        "\n",
        "masks, scores, logits = mask_predictor.predict(\n",
        "    box=box,\n",
        "    multimask_output=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV_JOjHBNnV5"
      },
      "source": [
        "### Results visualisation with Supervision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2opSwP1Np7s"
      },
      "outputs": [],
      "source": [
        "box_annotator = sv.BoxAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n",
        "mask_annotator = sv.MaskAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "detections = sv.Detections(\n",
        "    xyxy=sv.mask_to_xyxy(masks=masks),\n",
        "    mask=masks\n",
        ")\n",
        "detections = detections[detections.area == np.max(detections.area)]\n",
        "\n",
        "source_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "segmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=[source_image, segmented_image],\n",
        "    grid_size=(1, 2),\n",
        "    titles=['source image', 'segmented image']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD2RsmjSH5Kh"
      },
      "source": [
        "### Interaction with segmentation results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0Ts3OZ3MuzP"
      },
      "outputs": [],
      "source": [
        "import supervision as v\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=masks,\n",
        "    grid_size=(1, 4),\n",
        "    size=(16, 4)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S14Wh-hhr3wP"
      },
      "source": [
        "## Segment Anything in Roboflow Universe Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff80d5krPlkO"
      },
      "source": [
        "### Utils Supporting Dataset Processing\n",
        "\n",
        "A couple of helper functions that, unfortunately, we have to write ourselves to facilitate the processing of COCO annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZSU9BpHr2gc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Union, Optional\n",
        "from dataclasses_json import dataclass_json\n",
        "from supervision import Detections\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOCategory:\n",
        "    id: int\n",
        "    name: str\n",
        "    supercategory: str\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOImage:\n",
        "    id: int\n",
        "    width: int\n",
        "    height: int\n",
        "    file_name: str\n",
        "    license: int\n",
        "    date_captured: str\n",
        "    coco_url: Optional[str] = None\n",
        "    flickr_url: Optional[str] = None\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOAnnotation:\n",
        "    id: int\n",
        "    image_id: int\n",
        "    category_id: int\n",
        "    segmentation: List[List[float]]\n",
        "    area: float\n",
        "    bbox: Tuple[float, float, float, float]\n",
        "    iscrowd: int\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOLicense:\n",
        "    id: int\n",
        "    name: str\n",
        "    url: str\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOJson:\n",
        "    images: List[COCOImage]\n",
        "    annotations: List[COCOAnnotation]\n",
        "    categories: List[COCOCategory]\n",
        "    licenses: List[COCOLicense]\n",
        "\n",
        "\n",
        "def load_coco_json(json_file: str) -> COCOJson:\n",
        "    import json\n",
        "\n",
        "    with open(json_file, \"r\") as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    return COCOJson.from_dict(json_data)\n",
        "\n",
        "\n",
        "class COCOJsonUtility:\n",
        "    @staticmethod\n",
        "    def get_annotations_by_image_id(coco_data: COCOJson, image_id: int) -> List[COCOAnnotation]:\n",
        "        return [annotation for annotation in coco_data.annotations if annotation.image_id == image_id]\n",
        "\n",
        "    @staticmethod\n",
        "    def get_annotations_by_image_path(coco_data: COCOJson, image_path: str) -> Optional[List[COCOAnnotation]]:\n",
        "        image = COCOJsonUtility.get_image_by_path(coco_data, image_path)\n",
        "        if image:\n",
        "            return COCOJsonUtility.get_annotations_by_image_id(coco_data, image.id)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def get_image_by_path(coco_data: COCOJson, image_path: str) -> Optional[COCOImage]:\n",
        "        for image in coco_data.images:\n",
        "            if image.file_name == image_path:\n",
        "                return image\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def annotations2detections(annotations: List[COCOAnnotation]) -> Detections:\n",
        "        class_id, xyxy = [], []\n",
        "\n",
        "        for annotation in annotations:\n",
        "            x_min, y_min, width, height = annotation.bbox\n",
        "            class_id.append(annotation.category_id)\n",
        "            xyxy.append([\n",
        "                x_min,\n",
        "                y_min,\n",
        "                x_min + width,\n",
        "                y_min + height\n",
        "            ])\n",
        "\n",
        "        return Detections(\n",
        "            xyxy=np.array(xyxy, dtype=int),\n",
        "            class_id=np.array(class_id, dtype=int)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-jQ5c4TQAic"
      },
      "source": [
        "### Download Dataset from Roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxH9V2nHQC4M"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "\n",
        "import roboflow\n",
        "from roboflow import Roboflow\n",
        "\n",
        "roboflow.login()\n",
        "\n",
        "rf = Roboflow()\n",
        "\n",
        "project = rf.workspace(\"hashira-fhxpj\").project(\"mri-brain-tumor\")\n",
        "dataset = project.version(1).download(\"coco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-Xi4Dx0QuRL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "DATA_SET_SUBDIRECTORY = \"test\"\n",
        "ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\"\n",
        "IMAGES_DIRECTORY_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY)\n",
        "ANNOTATIONS_FILE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, ANNOTATIONS_FILE_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8b3oKP0QyiB"
      },
      "outputs": [],
      "source": [
        "coco_data = load_coco_json(json_file=ANNOTATIONS_FILE_PATH)\n",
        "\n",
        "CLASSES = [\n",
        "    category.name\n",
        "    for category\n",
        "    in coco_data.categories\n",
        "    if category.supercategory != 'none'\n",
        "]\n",
        "\n",
        "IMAGES = [\n",
        "    image.file_name\n",
        "    for image\n",
        "    in coco_data.images\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFQwR9-aQ0kx"
      },
      "outputs": [],
      "source": [
        "CLASSES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3WqT5qTQ6tI"
      },
      "source": [
        "### Single Image Bounding Box to Mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuUKYCLnRAaN"
      },
      "outputs": [],
      "source": [
        "# set random seed to allow easy reproduction of the experiment\n",
        "\n",
        "import random\n",
        "random.seed(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHw4yH8XRCo9"
      },
      "outputs": [],
      "source": [
        "EXAMPLE_IMAGE_NAME = random.choice(IMAGES)\n",
        "EXAMPLE_IMAGE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, EXAMPLE_IMAGE_NAME)\n",
        "\n",
        "# load dataset annotations\n",
        "annotations = COCOJsonUtility.get_annotations_by_image_path(coco_data=coco_data, image_path=EXAMPLE_IMAGE_NAME)\n",
        "ground_truth = COCOJsonUtility.annotations2detections(annotations=annotations)\n",
        "\n",
        "# small hack - coco numerate classes from 1, model from 0 + we drop first redundant class from coco json\n",
        "ground_truth.class_id = ground_truth.class_id - 1\n",
        "\n",
        "# load image\n",
        "image_bgr = cv2.imread(EXAMPLE_IMAGE_PATH)\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# initiate annotator\n",
        "box_annotator = sv.BoxAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n",
        "mask_annotator = sv.MaskAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "# annotate ground truth\n",
        "annotated_frame_ground_truth = box_annotator.annotate(scene=image_bgr.copy(), detections=ground_truth)\n",
        "\n",
        "# run SAM inference\n",
        "mask_predictor.set_image(image_rgb)\n",
        "\n",
        "masks, scores, logits = mask_predictor.predict(\n",
        "    box=ground_truth.xyxy[0],\n",
        "    multimask_output=True\n",
        ")\n",
        "\n",
        "detections = sv.Detections(\n",
        "    xyxy=sv.mask_to_xyxy(masks=masks),\n",
        "    mask=masks\n",
        ")\n",
        "detections = detections[detections.area == np.max(detections.area)]\n",
        "\n",
        "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=[annotated_frame_ground_truth, annotated_image],\n",
        "    grid_size=(1, 2),\n",
        "    titles=['source image', 'segmented image']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP9gEw9Tp8GJ"
      },
      "source": [
        "## 🏆 Congratulations\n",
        "\n",
        "### Learning Resources\n",
        "\n",
        "Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n",
        "\n",
        "- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.\n",
        "- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n",
        "- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n",
        "- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n",
        "\n",
        "### Convert data formats\n",
        "\n",
        "Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.\n",
        "\n",
        "### Connect computer vision to your project logic\n",
        "\n",
        "[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}